{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d424d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved X_test.npy and y_test.npy with shape: (452, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the CSVs\n",
    "# Load the data\n",
    "vehicle_df = pd.read_csv('/home/arjunrao/mini_proj_6/train_model/vehicle_data.csv')\n",
    "network_df = pd.read_csv('/home/arjunrao/mini_proj_6/train_model/network_data.csv')\n",
    "\n",
    "# Merge on `time_step` and `ap_id`\n",
    "vehicle_df['ap_id'] = vehicle_df['ap_id'].astype(str)\n",
    "network_df['ap_id'] = network_df['ap_id'].str.extract(r'(\\d+)').astype(str)  # clean `ap_1` to `1`\n",
    "merged_df = pd.merge(network_df, vehicle_df, on=['time_step', 'ap_id'])\n",
    "\n",
    "# Select relevant features\n",
    "features = ['avg_packet_rate', 'avg_latency', 'bandwidth_usage', 'speed', 'acceleration', 'active_nodes']\n",
    "merged_df = merged_df[[\"time_step\", \"ap_id\"] + features]\n",
    "# Normalize\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(merged_df[features])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 10\n",
    "X, y = [], []\n",
    "for i in range(len(df_scaled) - sequence_length):\n",
    "    X.append(df_scaled[i:i + sequence_length])\n",
    "    y.append(df_scaled[i + sequence_length])  \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Save test data\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "print(\"Saved X_test.npy and y_test.npy with shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f32075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 128)           69120     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118918 (464.52 KB)\n",
      "Trainable params: 118918 (464.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"network_congestion_lstm.h5\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be367a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 12:14:59.223864: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-09 12:14:59.227795: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-09 12:14:59.287902: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-09 12:14:59.289404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-09 12:15:00.237884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML prediction sender loop...\n",
      "[12:15:03] Sent batch prediction.\n",
      "[12:15:09] Sent batch prediction.\n",
      "[12:15:15] Sent batch prediction.\n",
      "[12:15:20] Sent batch prediction.\n",
      "[12:15:26] Sent batch prediction.\n",
      "[12:15:32] Sent batch prediction.\n",
      "[12:15:37] Sent batch prediction.\n",
      "[12:15:43] Sent batch prediction.\n",
      "[12:15:49] Sent batch prediction.\n",
      "[12:15:54] Sent batch prediction.\n",
      "[12:16:00] Sent batch prediction.\n",
      "[12:16:06] Sent batch prediction.\n",
      "[12:16:11] Sent batch prediction.\n",
      "[12:16:17] Sent batch prediction.\n",
      "[12:16:23] Sent batch prediction.\n",
      "[12:16:28] Sent batch prediction.\n",
      "[12:16:34] Sent batch prediction.\n",
      "[12:16:40] Sent batch prediction.\n",
      "[12:16:45] Sent batch prediction.\n",
      "[12:16:51] Sent batch prediction.\n",
      "[12:16:57] Sent batch prediction.\n",
      "[12:17:02] Sent batch prediction.\n",
      "[12:17:08] Sent batch prediction.\n",
      "[12:17:13] Sent batch prediction.\n",
      "[12:17:19] Sent batch prediction.\n",
      "[12:17:25] Sent batch prediction.\n",
      "[12:17:30] Sent batch prediction.\n",
      "[12:17:36] Sent batch prediction.\n",
      "[12:17:42] Sent batch prediction.\n",
      "[12:17:47] Sent batch prediction.\n",
      "[12:17:53] Sent batch prediction.\n",
      "[12:17:59] Sent batch prediction.\n",
      "[12:18:04] Sent batch prediction.\n",
      "[12:18:10] Sent batch prediction.\n",
      "[12:18:16] Sent batch prediction.\n",
      "[12:18:21] Sent batch prediction.\n",
      "[12:18:27] Sent batch prediction.\n",
      "[12:18:33] Sent batch prediction.\n",
      "[12:18:39] Sent batch prediction.\n",
      "[12:18:45] Sent batch prediction.\n",
      "[12:18:51] Sent batch prediction.\n",
      "[12:18:58] Sent batch prediction.\n",
      "[12:19:04] Sent batch prediction.\n",
      "[12:19:10] Sent batch prediction.\n",
      "[12:19:16] Sent batch prediction.\n",
      "[12:19:22] Sent batch prediction.\n",
      "[12:19:28] Sent batch prediction.\n",
      "[12:19:34] Sent batch prediction.\n",
      "[12:19:40] Sent batch prediction.\n",
      "[12:19:46] Sent batch prediction.\n",
      "[12:19:52] Sent batch prediction.\n",
      "[12:19:58] Sent batch prediction.\n",
      "[12:20:04] Sent batch prediction.\n",
      "[12:20:09] Sent batch prediction.\n",
      "[12:20:15] Sent batch prediction.\n",
      "[12:20:21] Sent batch prediction.\n",
      "[12:20:27] Sent batch prediction.\n",
      "[12:20:33] Sent batch prediction.\n",
      "[12:20:39] Sent batch prediction.\n",
      "[12:20:45] Sent batch prediction.\n",
      "[12:20:51] Sent batch prediction.\n",
      "[12:20:57] Sent batch prediction.\n",
      "[12:21:03] Sent batch prediction.\n",
      "[12:21:09] Sent batch prediction.\n",
      "[12:21:15] Sent batch prediction.\n",
      "[12:21:21] Sent batch prediction.\n",
      "[12:21:27] Sent batch prediction.\n",
      "[12:21:33] Sent batch prediction.\n",
      "[12:21:39] Sent batch prediction.\n",
      "[12:21:45] Sent batch prediction.\n",
      "[12:21:51] Sent batch prediction.\n",
      "[12:21:57] Sent batch prediction.\n",
      "[12:22:03] Sent batch prediction.\n",
      "[12:22:09] Sent batch prediction.\n",
      "[12:22:15] Sent batch prediction.\n",
      "[12:22:21] Sent batch prediction.\n",
      "[12:22:27] Sent batch prediction.\n",
      "[12:22:33] Sent batch prediction.\n",
      "[12:22:39] Sent batch prediction.\n",
      "[12:22:45] Sent batch prediction.\n",
      "[12:22:51] Sent batch prediction.\n",
      "[12:22:57] Sent batch prediction.\n",
      "[12:23:02] Sent batch prediction.\n",
      "[12:23:09] Sent batch prediction.\n",
      "[12:23:14] Sent batch prediction.\n",
      "[12:23:20] Sent batch prediction.\n",
      "[12:23:26] Sent batch prediction.\n",
      "[12:23:32] Sent batch prediction.\n",
      "[12:23:37] Sent batch prediction.\n",
      "[12:23:43] Sent batch prediction.\n",
      "[12:23:49] Sent batch prediction.\n",
      "[12:23:55] Sent batch prediction.\n",
      "[12:24:00] Sent batch prediction.\n",
      "[12:24:06] Sent batch prediction.\n",
      "[12:24:12] Sent batch prediction.\n",
      "[12:24:18] Sent batch prediction.\n",
      "[12:24:23] Sent batch prediction.\n",
      "[12:24:29] Sent batch prediction.\n",
      "[12:24:35] Sent batch prediction.\n",
      "[12:24:41] Sent batch prediction.\n",
      "[12:24:46] Sent batch prediction.\n",
      "[12:24:52] Sent batch prediction.\n",
      "[12:24:58] Sent batch prediction.\n",
      "[12:25:04] Sent batch prediction.\n",
      "[12:25:10] Sent batch prediction.\n",
      "[12:25:16] Sent batch prediction.\n",
      "[12:25:22] Sent batch prediction.\n",
      "[12:25:27] Sent batch prediction.\n",
      "[12:25:33] Sent batch prediction.\n",
      "[12:25:39] Sent batch prediction.\n",
      "[12:25:45] Sent batch prediction.\n",
      "[12:25:51] Sent batch prediction.\n",
      "[12:25:56] Sent batch prediction.\n",
      "[12:26:02] Sent batch prediction.\n",
      "[12:26:08] Sent batch prediction.\n",
      "[12:26:14] Sent batch prediction.\n",
      "[12:26:19] Sent batch prediction.\n",
      "[12:26:25] Sent batch prediction.\n",
      "[12:26:31] Sent batch prediction.\n",
      "[12:26:37] Sent batch prediction.\n",
      "[12:26:42] Sent batch prediction.\n",
      "[12:26:48] Sent batch prediction.\n",
      "[12:26:55] Sent batch prediction.\n",
      "[12:27:01] Sent batch prediction.\n",
      "[12:27:06] Sent batch prediction.\n",
      "[12:27:12] Sent batch prediction.\n",
      "[12:27:18] Sent batch prediction.\n",
      "[12:27:24] Sent batch prediction.\n",
      "[12:27:30] Sent batch prediction.\n",
      "[12:27:35] Sent batch prediction.\n",
      "[12:27:41] Sent batch prediction.\n",
      "[12:27:47] Sent batch prediction.\n",
      "[12:27:53] Sent batch prediction.\n",
      "[12:27:58] Sent batch prediction.\n",
      "[12:28:04] Sent batch prediction.\n",
      "[12:28:10] Sent batch prediction.\n",
      "[12:28:16] Sent batch prediction.\n",
      "[12:28:22] Sent batch prediction.\n",
      "[12:28:27] Sent batch prediction.\n",
      "[12:28:33] Sent batch prediction.\n",
      "[12:28:39] Sent batch prediction.\n",
      "[12:28:45] Sent batch prediction.\n",
      "[12:28:51] Sent batch prediction.\n",
      "[12:28:56] Sent batch prediction.\n",
      "[12:29:02] Sent batch prediction.\n",
      "[12:29:08] Sent batch prediction.\n",
      "[12:29:35] Sent batch prediction.\n",
      "[12:29:41] Sent batch prediction.\n",
      "[12:29:47] Sent batch prediction.\n",
      "[12:29:52] Sent batch prediction.\n",
      "[12:29:58] Sent batch prediction.\n",
      "[12:30:09] Sent batch prediction.\n",
      "[12:30:15] Sent batch prediction.\n",
      "[12:30:21] Sent batch prediction.\n",
      "[12:30:26] Sent batch prediction.\n",
      "[12:30:32] Sent batch prediction.\n",
      "[12:36:22] Sent batch prediction.\n",
      "[12:36:28] Sent batch prediction.\n",
      "[12:36:34] Sent batch prediction.\n",
      "[12:36:39] Sent batch prediction.\n",
      "[12:36:45] Sent batch prediction.\n",
      "[13:09:24] Sent batch prediction.\n",
      "[13:09:29] Sent batch prediction.\n",
      "[13:09:35] Sent batch prediction.\n",
      "[13:09:41] Sent batch prediction.\n",
      "[13:09:47] Sent batch prediction.\n",
      "[13:13:34] Sent batch prediction.\n",
      "[13:13:40] Sent batch prediction.\n",
      "[13:13:46] Sent batch prediction.\n",
      "[13:13:52] Sent batch prediction.\n",
      "[13:13:58] Sent batch prediction.\n",
      "[13:14:16] Sent batch prediction.\n",
      "[13:14:22] Sent batch prediction.\n",
      "[13:14:27] Sent batch prediction.\n",
      "[13:14:33] Sent batch prediction.\n",
      "[13:14:39] Sent batch prediction.\n",
      "[13:14:50] Sent batch prediction.\n",
      "[13:14:56] Sent batch prediction.\n",
      "[13:15:02] Sent batch prediction.\n",
      "[13:15:08] Sent batch prediction.\n",
      "[13:15:13] Sent batch prediction.\n",
      "[13:18:33] Sent batch prediction.\n",
      "[13:18:39] Sent batch prediction.\n",
      "[13:18:45] Sent batch prediction.\n",
      "[13:18:51] Sent batch prediction.\n",
      "[13:18:56] Sent batch prediction.\n",
      "[13:20:34] Sent batch prediction.\n",
      "[13:20:39] Sent batch prediction.\n",
      "[13:20:45] Sent batch prediction.\n",
      "[13:20:51] Sent batch prediction.\n",
      "[13:20:57] Sent batch prediction.\n",
      "[13:21:39] Sent batch prediction.\n",
      "[13:21:45] Sent batch prediction.\n",
      "[13:21:51] Sent batch prediction.\n",
      "[13:21:57] Sent batch prediction.\n",
      "[13:22:02] Sent batch prediction.\n",
      "[13:22:08] Sent batch prediction.\n",
      "[13:22:14] Sent batch prediction.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import socket\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load LSTM model\n",
    "model = load_model(\"network_congestion_lstm.h5\")\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "\n",
    "# Feature labels used (same order as during training)\n",
    "feature_labels = ['avg_packet_rate', 'avg_latency', 'bandwidth_usage', 'speed', 'acceleration', 'active_nodes']\n",
    "\n",
    "# Set up UDP\n",
    "controller_ip = \"127.0.0.1\"\n",
    "controller_port = 9999\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "print(\"Starting ML prediction sender loop...\")\n",
    "i = 0\n",
    "while True:\n",
    "    all_ap_predictions = []\n",
    "\n",
    "    for ap_id in range(1, 11):\n",
    "        if i >= X_test.shape[0]:\n",
    "            i = 0\n",
    "\n",
    "        x_input = X_test[i:i+1]\n",
    "        prediction = model.predict(x_input, verbose=0)[0]  # Shape (6,)\n",
    "\n",
    "        predicted_features = {\n",
    "            feature_labels[j]: round(float(prediction[j]), 4)\n",
    "            for j in range(len(feature_labels))\n",
    "        }\n",
    "\n",
    "        ap_data = {\n",
    "            \"ap_id\": ap_id,\n",
    "            \"predicted_features\": predicted_features\n",
    "        }\n",
    "\n",
    "        all_ap_predictions.append(ap_data)\n",
    "        i += 1\n",
    "\n",
    "    payload = {\n",
    "        \"type\": \"batch\",\n",
    "        \"data\": all_ap_predictions\n",
    "    }\n",
    "\n",
    "    sock.sendto(json.dumps(payload).encode(), (controller_ip, controller_port))\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Sent batch prediction.\")\n",
    "\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b533d4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"batch\", \"data\": [{\"ap_id\": 1, \"mac\": \"00:00:00:00:00:01\", \"congestion_score\": 0.2959, \"avg_packet_rate\": 0.312, \"avg_latency\": 0.1938, \"bandwidth_usage\": 0.3764, \"speed\": 0.4976, \"acceleration\": 0.5483, \"active_nodes\": 0.6054}, {\"ap_id\": 2, \"mac\": \"00:00:00:00:00:02\", \"congestion_score\": 0.4786, \"avg_packet_rate\": 0.5296, \"avg_latency\": 0.3841, \"bandwidth_usage\": 0.505, \"speed\": 0.4943, \"acceleration\": 0.5578, \"active_nodes\": 0.8528}, {\"ap_id\": 3, \"mac\": \"00:00:00:00:00:03\", \"congestion_score\": 0.4803, \"avg_packet_rate\": 0.5042, \"avg_latency\": 0.4552, \"bandwidth_usage\": 0.4734, \"speed\": 0.5007, \"acceleration\": 0.5643, \"active_nodes\": 0.3438}, {\"ap_id\": 4, \"mac\": \"00:00:00:00:00:04\", \"congestion_score\": 0.5901, \"avg_packet_rate\": 0.7303, \"avg_latency\": 0.4, \"bandwidth_usage\": 0.5933, \"speed\": 0.5009, \"acceleration\": 0.5663, \"active_nodes\": 0.612}, {\"ap_id\": 5, \"mac\": \"00:00:00:00:00:05\", \"congestion_score\": 0.4738, \"avg_packet_rate\": 0.3654, \"avg_latency\": 0.4826, \"bandwidth_usage\": 0.6097, \"speed\": 0.4871, \"acceleration\": 0.5583, \"active_nodes\": -0.0155}, {\"ap_id\": 6, \"mac\": \"00:00:00:00:00:06\", \"congestion_score\": 0.5794, \"avg_packet_rate\": 0.6995, \"avg_latency\": 0.2781, \"bandwidth_usage\": 0.7206, \"speed\": 0.504, \"acceleration\": 0.5693, \"active_nodes\": 0.0634}, {\"ap_id\": 7, \"mac\": \"00:00:00:00:00:07\", \"congestion_score\": 0.5825, \"avg_packet_rate\": 0.6464, \"avg_latency\": 0.4444, \"bandwidth_usage\": 0.6355, \"speed\": 0.4998, \"acceleration\": 0.5644, \"active_nodes\": 0.2142}, {\"ap_id\": 8, \"mac\": \"00:00:00:00:00:08\", \"congestion_score\": 0.6089, \"avg_packet_rate\": 0.5729, \"avg_latency\": 0.8662, \"bandwidth_usage\": 0.3995, \"speed\": 0.5039, \"acceleration\": 0.5804, \"active_nodes\": 0.0868}, {\"ap_id\": 9, \"mac\": \"00:00:00:00:00:09\", \"congestion_score\": 0.5681, \"avg_packet_rate\": 0.5839, \"avg_latency\": 0.5374, \"bandwidth_usage\": 0.5777, \"speed\": 0.4947, \"acceleration\": 0.566, \"active_nodes\": 0.8468}, {\"ap_id\": 10, \"mac\": \"00:00:00:00:00:0A\", \"congestion_score\": 0.4947, \"avg_packet_rate\": 0.4864, \"avg_latency\": 0.5343, \"bandwidth_usage\": 0.4662, \"speed\": 0.4988, \"acceleration\": 0.5653, \"active_nodes\": 0.3431}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2141"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(json.dumps(payload))  # Log the payload to see what is being sent\n",
    "sock.sendto(json.dumps(payload).encode(), (controller_ip, controller_port))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c9704b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_packet_rate': 0.4864, 'avg_latency': 0.5343, 'bandwidth_usage': 0.4662, 'speed': 0.4988, 'acceleration': 0.5653, 'active_nodes': 0.3431}\n"
     ]
    }
   ],
   "source": [
    "print(predicted_features)  # Log to check the values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
